{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1-Shot Classifier\n",
    "\n",
    "Classify images directly. Deep resnet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# system libraries\n",
    "import os\n",
    "from glob import glob\n",
    "import logging\n",
    "\n",
    "# numerical,image and plotting stuff\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from skimage import io\n",
    "import skimage.transform as tf\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style(\"whitegrid\", {'axes.grid' : False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get all image file names and associated class from their latest location\n",
    "class_folders = glob('data/train/*')\n",
    "files = [glob(cls + '/*') for cls in class_folders] # put class info with file name\n",
    "files = [img for cls in files for img in cls]\n",
    "df = pd.DataFrame({'fpath':files})\n",
    "df['category'] = df.fpath.str.extract('data/train/([a-zA-Z]*)/img', expand=False) # extract class\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def read_and_augment(fpath,rng,w_out=512,h_out=512):\n",
    "    # read image\n",
    "    img = io.imread(fpath)\n",
    "    h, w, _ = img.shape\n",
    "    # prepare transformations\n",
    "    r_tx, r_ty = -w/2, -h/2\n",
    "    r_rotate = rng.uniform(-np.pi/6,np.pi/6,1)[0]\n",
    "    r_scale = rng.uniform(0.9,1.1,1)\n",
    "    tf_rotate = tf.SimilarityTransform(rotation=r_rotate)\n",
    "    tf_scale = tf.SimilarityTransform(scale=r_scale)\n",
    "    tf_shear = tf.AffineTransform(shear=rng.uniform(-0.1,0.1,1))\n",
    "    tf_shift = tf.SimilarityTransform(translation=[-r_tx, -r_ty])\n",
    "    tf_shift_inv = tf.SimilarityTransform(translation=[r_tx, r_ty])\n",
    "    trans = tf_shift + tf_scale + tf_shear + tf_rotate + tf_shift_inv\n",
    "\n",
    "    img_warped = tf.warp(img, trans.inverse)\n",
    "    img_resized = tf.resize(img_warped,(h_out,w_out))/128.-1\n",
    "    # randomly flip horizontally and vertically\n",
    "    if rng.uniform(0,1) < 0.5:\n",
    "        img_resized = img_resized[::-1,:,:]\n",
    "    if rng.uniform(0,1) < 0.5:\n",
    "        img_resized = img_resized[:,::-1,:]\n",
    "    return img_resized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen(df,batch_size=64,rng=None,h_out=512,w_out=512,ch_out=3):\n",
    "    \"\"\"\n",
    "    This generator produces a batch of (X,y) for training a fish detector.\n",
    "    Images are randomly augmented.\n",
    "    \n",
    "    Inputs:\n",
    "    df is a pandas dataframe containing the file path (fpath) and class\n",
    "    \n",
    "    Outputs:\n",
    "    X is a 4D tensor of shape (batch_size,h,w,ch)\n",
    "    y is a 2D vector of (batch_size,8), for the 8 classes\n",
    "    \"\"\"\n",
    "\n",
    "    if rng is None:\n",
    "        rng = np.random.RandomState()\n",
    "\n",
    "    n = len(df)\n",
    "    total_batch = int(np.ceil(n / batch_size))\n",
    "    logging.info('generating %d batches with %d samples per epoch' % (total_batch,n))\n",
    "    \n",
    "    # initialize labelBinarizer\n",
    "    ohc = LabelBinarizer()\n",
    "    ohc.fit(df.category)\n",
    "\n",
    "    while True:\n",
    "        # shuffle examples every epoch\n",
    "        df_shuffled = df.iloc[rng.permutation(n)]\n",
    "        for i_batch in range(total_batch):\n",
    "            # limit end index by size of df_gen to prevent \n",
    "            # indexing up to the next multiple of batch_size\n",
    "            i_start, i_end = i_batch * batch_size, min((i_batch + 1) * batch_size,n)\n",
    "            i_batch_size = i_end - i_start\n",
    "            X = np.zeros((i_batch_size,h_out,w_out,ch_out))\n",
    "            Y = np.zeros((i_batch_size,8))\n",
    "            for i in range(i_start,i_end):\n",
    "                fpath = df_shuffled.iloc[i]['fpath']\n",
    "                x = read_and_augment(fpath,rng,w_out,h_out) # read and augment image\n",
    "                i_intrabatch = i - i_start\n",
    "                X[i_intrabatch,...] = x\n",
    "            Y = ohc.transform(df_shuffled.iloc[i_start:i_end]['category'])\n",
    "            logging.info('yielding batch %d of size %d' % (i_batch, i_batch_size))\n",
    "            yield(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# initialize generators\n",
    "rng = np.random.RandomState(290615)\n",
    "\n",
    "# we just use train valid test for training our model\n",
    "ix_coord = rng.choice(range(3),p=[0.8,0.1,0.1],size=len(df))\n",
    "df_train = df.ix[ix_coord==0]\n",
    "df_valid = df.ix[ix_coord==1]\n",
    "df_test = df.ix[ix_coord==2]\n",
    "gn_train = gen(df_train,batch_size=64,rng=np.random.RandomState(290615),h_out=512,w_out=512)\n",
    "gn_valid = gen(df_valid,batch_size=64,rng=np.random.RandomState(290615),h_out=512,w_out=512)\n",
    "samples_per_epoch = len(df_train)\n",
    "nb_val_samples = len(df_valid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.layers import Input, Convolution2D, MaxPooling2D, ZeroPadding2D, Layer,\\\n",
    "    Activation, Dropout, Flatten, AveragePooling2D, Dense, merge\n",
    "from keras.models import Model\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.regularizers import l2\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as K\n",
    "from keras.callbacks import TensorBoard, ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "# setup model\n",
    "h_out, w_out = 512, 512\n",
    "\n",
    "# creates a residual block\n",
    "def res_block(input_layer,depth,layer_number,if_pool=True):\n",
    "    # skip bn-relu for first layer (0-indexed here)\n",
    "    if layer_number > 0:\n",
    "        bn = BatchNormalization()(input_layer)\n",
    "        bn_relu = Activation('relu')(bn)\n",
    "    else:\n",
    "        bn_relu = input_layer\n",
    "    bn_relu_conv = Convolution2D(depth,3,3,name='conv'+str(layer_number)+'_1',\n",
    "                               border_mode='same',W_regularizer=l2(0.0001))(bn_relu)\n",
    "    bn_relu_conv_bn = BatchNormalization()(bn_relu_conv)\n",
    "    bn_relu_conv_bn_relu = Activation('relu')(bn_relu_conv_bn)\n",
    "    bn_relu_conv_bn_relu_conv = Convolution2D(depth,3,3,name='conv'+str(layer_number)+'_2',\n",
    "                               border_mode='same',W_regularizer=l2(0.0001))(bn_relu_conv_bn_relu)\n",
    "    residual = Convolution2D(depth,1,1,name='resid'+str(layer_number),border_mode='same',\n",
    "                             W_regularizer=l2(0.0001))(input_layer)\n",
    "    merged = merge([bn_relu_conv_bn_relu_conv,residual],mode='sum')\n",
    "    merged_pool = MaxPooling2D((2,2),strides=(2,2))(merged)\n",
    "    return merged_pool\n",
    "\n",
    "\n",
    "def new_model(h_in=512, w_in=512 ,ch=3):\n",
    "    \n",
    "    image_input = Input(shape=(512,512,3))\n",
    "\n",
    "    res1_out = res_block(image_input,16,0,True)\n",
    "    res2_out = res_block(res1_out,32,1,True)\n",
    "    res3_out = res_block(res2_out,64,2,True)\n",
    "    res4_out = res_block(res3_out,128,3,True)\n",
    "    res5_out = res_block(res4_out,128,4,False)\n",
    "    res6_out = res_block(res5_out,256,5,True)\n",
    "    res7_out = res_block(res6_out,512,6,False)\n",
    "    res7_avg = AveragePooling2D(pool_size=(4,4))(res7_out)\n",
    "    post_conv_flat = Flatten()(res5_out)\n",
    "\n",
    "    post_conv_flat = Dropout(0.5)(post_conv_flat)\n",
    "    post_conv_flat = Dense(1024, activation='relu', init='glorot_normal')(post_conv_flat)\n",
    "    post_conv_flat = Dropout(0.5)(post_conv_flat)\n",
    "    predictions = Dense(8, activation='softmax', init='glorot_normal')(post_conv_flat)\n",
    "    \n",
    "    model = Model(input=[image_input],output=[predictions])\n",
    "    return model\n",
    "\n",
    "model = new_model()\n",
    "adam = Adam(lr=0.001)\n",
    "model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "# setup tensorboard graph directory\n",
    "graph_dir = 'graph/graph_1shot'\n",
    "if not os.path.exists(graph_dir):\n",
    "    os.makedirs(graph_dir)\n",
    "\n",
    "# prepare callbacks\n",
    "tb = TensorBoard(log_dir=graph_dir, write_graph=True, write_images=True)\n",
    "mc_coord = ModelCheckpoint(filepath='models/classifier_ep{epoch:02d}_loss{val_loss:.2f}_acc{val_acc:.2f}.h5',\n",
    "                           verbose=1,save_best_only=True)\n",
    "reducelr = ReduceLROnPlateau(monitor='val_loss', factor=0.9, patience=5, \n",
    "                             verbose=1, mode='min', epsilon=0.0001, cooldown=0, min_lr=1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.fit_generator(generator=gn_train,\n",
    "                        samples_per_epoch=samples_per_epoch,\n",
    "                        validation_data=gn_valid,\n",
    "                        nb_val_samples=nb_val_samples,\n",
    "                        nb_epoch=500,\n",
    "                        callbacks=[tb,mc_coord,reducelr])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read in data files\n",
    "files = glob('data/test_stg2/*')\n",
    "df = pd.DataFrame({'fpath':files,'hx':np.nan,'hy':np.nan,'tx':np.nan,'ty':np.nan})\n",
    "print(len(df))\n",
    "df.head()\n",
    "\n",
    "model.load_weights('models/classifier_ep126_loss0.17_acc0.95.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
