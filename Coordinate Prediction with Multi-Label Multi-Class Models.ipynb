{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real-Valued Coordinate Prediction Using Multi-Label Multi-Class Models\n",
    "\n",
    "This notebook aims to predict coordinates using multi-label multi-class models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# system libraries\n",
    "import os\n",
    "from glob import glob\n",
    "import logging\n",
    "\n",
    "# numerical,image and plotting stuff\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from skimage import io\n",
    "import skimage.transform as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style(\"whitegrid\", {'axes.grid' : False})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a length-w\\*w vector which we want to get the x and y coordinates of the entry with the highest signal.\n",
    "\n",
    "x1, x2,              ... xw,\n",
    "\n",
    "xw+1, xw+2,          ... x2w,\n",
    "\n",
    "...\n",
    "\n",
    "x(w-1)w+1, x(w-1)w+2,... xw\\*w\n",
    "\n",
    "y1 would be a w-vector indicating the x coordinate with the signal, y2 would be the corresponding w-vector for the y-coordinate.\n",
    "\n",
    "we will train the inputs on 2 fc layers and then learn the coordinates. Each output y would have its own softmax stacked on top of the final fc layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(0)\n",
    "n = 1000000\n",
    "w = 10\n",
    "X = np.round(rng.uniform(low=-1,high=1,size=(n,w*w)),decimals=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is how our input data will look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.1 ,  0.43,  0.21,  0.09, -0.15,  0.29, -0.12,  0.78,  0.93,\n",
       "         -0.23],\n",
       "        [ 0.58,  0.06,  0.14,  0.85, -0.86, -0.83, -0.96,  0.67,  0.56,\n",
       "          0.74],\n",
       "        [ 0.96,  0.6 , -0.08,  0.56, -0.76,  0.28, -0.71,  0.89,  0.04,\n",
       "         -0.17],\n",
       "        [-0.47,  0.55, -0.09,  0.14, -0.96,  0.24,  0.22,  0.23,  0.89,\n",
       "          0.36],\n",
       "        [-0.28, -0.13,  0.4 , -0.88,  0.33,  0.34, -0.58, -0.74, -0.37,\n",
       "         -0.27],\n",
       "        [ 0.14, -0.12,  0.98, -0.8 , -0.58, -0.68,  0.31, -0.49, -0.07,\n",
       "         -0.51],\n",
       "        [-0.68, -0.78,  0.31, -0.72, -0.61, -0.26,  0.64, -0.81,  0.68,\n",
       "         -0.81],\n",
       "        [ 0.95, -0.06,  0.95,  0.21,  0.48, -0.92, -0.43, -0.76, -0.41,\n",
       "         -0.76],\n",
       "        [-0.36, -0.17, -0.87,  0.38,  0.13, -0.47,  0.05, -0.81,  0.15,\n",
       "          0.86],\n",
       "        [-0.36,  0.33, -0.74,  0.43, -0.42, -0.63,  0.17, -0.96,  0.66,\n",
       "         -0.99]],\n",
       "\n",
       "       [[ 0.36, -0.46,  0.47,  0.92, -0.5 ,  0.15,  0.18,  0.14, -0.55,\n",
       "          0.91],\n",
       "        [-0.11,  0.69,  0.4 , -0.41,  0.63, -0.21,  0.76,  0.16,  0.76,\n",
       "          0.39],\n",
       "        [ 0.45,  0.  ,  0.91,  0.29, -0.15,  0.21, -0.96, -0.4 ,  0.32,\n",
       "         -0.42],\n",
       "        [ 0.24, -0.14, -0.73, -0.4 ,  0.14,  0.18,  0.15,  0.31,  0.3 ,\n",
       "         -0.14],\n",
       "        [ 0.79, -0.26, -0.13,  0.78,  0.61,  0.41, -0.8 ,  0.84,  0.43,  1.  ],\n",
       "        [-0.7 ,  0.74, -0.68,  0.23, -0.75,  0.7 ,  0.61,  0.14, -0.19,\n",
       "         -0.86],\n",
       "        [ 0.39, -0.09,  0.44,  0.73,  0.95,  0.71, -0.98, -0.28,  0.46,\n",
       "         -0.66],\n",
       "        [ 0.04, -0.89, -0.6 , -0.96,  0.59, -0.55, -0.31,  0.86,  0.41,\n",
       "         -0.94],\n",
       "        [-0.67,  0.24,  0.15, -0.52,  0.87,  0.23,  0.07,  0.18,  0.46,\n",
       "         -0.38],\n",
       "        [-0.2 , -0.58, -0.63,  0.89,  0.48, -0.02, -0.55, -0.49, -0.88,\n",
       "         -0.13]],\n",
       "\n",
       "       [[-0.38,  0.39, -0.24, -0.64, -0.95, -0.87,  0.36, -0.09,  0.07,\n",
       "          0.79],\n",
       "        [ 0.98, -0.57,  0.33, -0.47, -0.96,  0.52, -0.36, -0.23,  0.18,\n",
       "          0.66],\n",
       "        [ 0.26,  0.75, -0.45,  0.6 , -0.63,  0.91,  0.37, -0.57,  0.89,\n",
       "          0.46],\n",
       "        [-0.49, -0.57,  0.04, -0.95, -0.59, -0.15, -0.25, -0.07, -0.44,\n",
       "          0.17],\n",
       "        [ 0.73, -0.76,  0.03, -0.74,  0.43, -0.21,  0.13, -0.63, -0.71,\n",
       "         -0.02],\n",
       "        [-0.29,  0.88,  0.53,  0.5 ,  0.81, -0.83,  0.1 ,  0.17,  0.92,\n",
       "         -0.42],\n",
       "        [-0.52, -0.8 , -0.97,  0.86,  0.34,  0.57, -0.44,  0.17, -0.87,\n",
       "         -0.03],\n",
       "        [ 0.95,  0.75, -0.32,  0.92, -0.54,  0.9 ,  0.88,  0.6 ,  0.26,\n",
       "          0.75],\n",
       "        [-0.41,  0.7 ,  0.24, -0.97, -0.31, -0.7 ,  0.96, -0.04, -0.01,\n",
       "          0.28],\n",
       "        [-0.26, -0.73,  0.64, -0.62,  0.02, -0.55, -0.8 ,  0.72,  0.95,\n",
       "          0.92]],\n",
       "\n",
       "       [[ 0.81,  0.55, -0.33, -0.84, -0.19, -0.54, -0.74, -0.89,  0.45,\n",
       "         -0.98],\n",
       "        [ 0.54, -0.71, -0.84, -0.82,  0.34, -0.51, -0.16,  0.11,  0.72,\n",
       "          0.45],\n",
       "        [-0.46, -0.74, -0.89, -0.4 , -0.48, -0.09,  0.37,  0.39, -0.43,\n",
       "         -0.24],\n",
       "        [-0.64,  0.58, -0.89,  0.39,  0.56,  0.55, -0.48, -0.25,  0.18,\n",
       "         -0.45],\n",
       "        [-0.26, -0.61, -0.08, -0.91,  0.6 , -0.85,  0.04, -0.39,  0.16,\n",
       "          0.92],\n",
       "        [ 0.29, -0.93, -0.14,  0.02,  0.07,  0.36, -0.44, -0.74, -0.21,\n",
       "          0.91],\n",
       "        [-0.63,  0.81,  0.09, -0.09,  0.76, -0.08,  0.45, -0.2 ,  0.81,\n",
       "          0.38],\n",
       "        [ 0.4 , -0.34,  0.51,  0.27, -0.52, -0.68,  0.59,  0.92, -0.08,\n",
       "          0.18],\n",
       "        [ 0.72, -0.09,  0.9 ,  0.15,  0.64,  0.82,  0.63, -0.68,  0.26,\n",
       "         -0.2 ],\n",
       "        [-0.87, -0.15, -0.48,  0.7 , -0.93,  0.92, -0.29, -0.29, -0.97,\n",
       "         -0.63]],\n",
       "\n",
       "       [[-0.2 ,  0.86, -0.8 ,  0.89,  0.74, -0.09, -0.35, -0.53,  0.23,\n",
       "         -0.93],\n",
       "        [-0.97, -0.14, -0.86, -0.5 , -0.56, -0.49, -0.74, -0.98, -0.77,\n",
       "          0.24],\n",
       "        [ 0.95,  0.98, -0.18, -0.67,  0.28, -0.02,  0.98, -0.87,  0.57,\n",
       "         -0.42],\n",
       "        [-0.52,  0.33, -0.51,  0.33,  0.03, -0.15,  0.11, -0.43,  0.41,\n",
       "         -0.17],\n",
       "        [-0.28,  0.66,  0.85, -0.91, -0.53, -0.3 ,  0.63,  0.97,  0.94,\n",
       "          0.81],\n",
       "        [-0.41,  0.98, -0.5 , -0.79,  0.9 , -0.53,  0.38, -0.88,  0.46,\n",
       "          0.76],\n",
       "        [-0.46, -0.24, -0.25,  0.5 , -0.52, -0.66, -0.1 , -0.39,  0.68,\n",
       "         -0.52],\n",
       "        [ 0.  ,  0.89,  0.27,  0.73,  0.88,  0.5 ,  0.4 ,  0.94,  0.99,\n",
       "         -0.1 ],\n",
       "        [-0.86, -0.41, -0.7 , -0.17, -0.74,  0.21, -0.23,  0.79,  0.94,\n",
       "          0.09],\n",
       "        [-0.45,  0.18,  0.79, -0.19,  0.1 , -0.46, -0.09, -0.2 , -0.5 ,\n",
       "          0.01]]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0:5,:].reshape((5,w,w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5]\n",
      " [4]\n",
      " [1]\n",
      " [4]\n",
      " [7]]\n",
      "[[2]\n",
      " [9]\n",
      " [0]\n",
      " [9]\n",
      " [8]]\n"
     ]
    }
   ],
   "source": [
    "# prepare Y outputs for regression\n",
    "y1 = np.int16(X.argmax(axis=1)/w).reshape((n,1))\n",
    "y2 = np.int16(X.argmax(axis=1)%w).reshape((n,1))\n",
    "print(y1[0:5])\n",
    "print(y2[0:5])\n",
    "Y_reg = np.hstack((y1,y2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  0.  0.  0.  1.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.]]\n",
      "[[ 0.  0.  1.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  1.  0.]]\n",
      "[[ 0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  1.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   1.  0.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "y1 = np.int16(X.argmax(axis=1)/w).reshape((n,1))\n",
    "y2 = np.int16(X.argmax(axis=1)%w).reshape((n,1))\n",
    "enc = OneHotEncoder(sparse=False)\n",
    "enc.fit(np.arange(w).reshape(w,1))\n",
    "y1 = enc.transform(y1)\n",
    "y2 = enc.transform(y2)\n",
    "print(y1[0:5,:])\n",
    "print(y2[0:5,:])\n",
    "Y_sm = np.hstack([y1,y2])\n",
    "Y_sm_half = np.hstack([y1,y2])\n",
    "print(Y_sm[0:5,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by testing a model with a softmax on the top, combining over both coordinate outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.layers.pooling import MaxPooling1D\n",
    "from keras.optimizers import SGD, RMSprop\n",
    "\n",
    "m_sm = Sequential([\n",
    "    Dense(16, input_dim=w*w),\n",
    "    Activation('tanh'),\n",
    "    Dropout(0.2),\n",
    "    Dense(16, input_dim=8),\n",
    "    Activation('tanh'),\n",
    "    Dropout(0.2),\n",
    "    Dense(2*w),\n",
    "    Activation('softmax'),\n",
    "])\n",
    "\n",
    "m_reg = Sequential([\n",
    "    Dense(16, input_dim=w*w),\n",
    "    Activation('tanh'),\n",
    "    Dropout(0.2),\n",
    "    Dense(16, input_dim=8),\n",
    "    Activation('tanh'),\n",
    "    Dropout(0.2),\n",
    "    Dense(2),\n",
    "    Activation('linear'),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 800000 samples, validate on 200000 samples\n",
      "Epoch 1/3\n",
      "800000/800000 [==============================] - 40s - loss: 8.0633 - mean_absolute_error: 2.4492 - val_loss: 8.0113 - val_mean_absolute_error: 2.4454\n",
      "Epoch 2/3\n",
      "800000/800000 [==============================] - 43s - loss: 8.0324 - mean_absolute_error: 2.4448 - val_loss: 7.9986 - val_mean_absolute_error: 2.4420\n",
      "Epoch 3/3\n",
      "800000/800000 [==============================] - 41s - loss: 8.0301 - mean_absolute_error: 2.4444 - val_loss: 8.0096 - val_mean_absolute_error: 2.4419\n",
      "Train on 800000 samples, validate on 200000 samples\n",
      "Epoch 1/3\n",
      "800000/800000 [==============================] - 38s - loss: 7.9964 - mean_absolute_error: 2.4395 - val_loss: 7.9816 - val_mean_absolute_error: 2.4373\n",
      "Epoch 2/3\n",
      "800000/800000 [==============================] - 35s - loss: 7.9910 - mean_absolute_error: 2.4385 - val_loss: 7.9808 - val_mean_absolute_error: 2.4378\n",
      "Epoch 3/3\n",
      "800000/800000 [==============================] - 36s - loss: 7.9900 - mean_absolute_error: 2.4382 - val_loss: 7.9833 - val_mean_absolute_error: 2.4388\n",
      "Train on 800000 samples, validate on 200000 samples\n",
      "Epoch 1/3\n",
      "800000/800000 [==============================] - 33s - loss: 7.9860 - mean_absolute_error: 2.4379 - val_loss: 7.9786 - val_mean_absolute_error: 2.4370\n",
      "Epoch 2/3\n",
      "800000/800000 [==============================] - 35s - loss: 7.9856 - mean_absolute_error: 2.4377 - val_loss: 7.9790 - val_mean_absolute_error: 2.4375\n",
      "Epoch 3/3\n",
      "800000/800000 [==============================] - 37s - loss: 7.9862 - mean_absolute_error: 2.4378 - val_loss: 7.9787 - val_mean_absolute_error: 2.4370\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x11b286748>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train regression model\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "m_reg.compile(optimizer=sgd,loss='mean_squared_error',metrics=['mae'])\n",
    "m_reg.fit(x=X,y=Y_reg,batch_size=64,nb_epoch=2,validation_split=0.2)\n",
    "sgd = SGD(lr=0.001, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "m_reg.layers[2] = Dropout(0.)\n",
    "m_reg.layers[5] = Dropout(0.)\n",
    "m_reg.compile(optimizer=sgd,loss='mean_squared_error',metrics=['mae'])\n",
    "m_reg.fit(x=X,y=Y_reg,batch_size=64,nb_epoch=1,validation_split=0.2)\n",
    "sgd = SGD(lr=0.0001, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "m_reg.compile(optimizer=sgd,loss='mean_squared_error',metrics=['mae'])\n",
    "m_reg.fit(x=X,y=Y_reg,batch_size=64,nb_epoch=1,validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 800000 samples, validate on 200000 samples\n",
      "Epoch 1/2\n",
      "800000/800000 [==============================] - 45s - loss: 5.8559 - acc: 0.0961 - val_loss: 5.8100 - val_acc: 0.1172\n",
      "Epoch 2/2\n",
      "800000/800000 [==============================] - 45s - loss: 5.8555 - acc: 0.0940 - val_loss: 5.8089 - val_acc: 0.1020\n",
      "Train on 800000 samples, validate on 200000 samples\n",
      "Epoch 1/1\n",
      "800000/800000 [==============================] - 44s - loss: 5.8468 - acc: 0.0938 - val_loss: 5.7975 - val_acc: 0.1097\n",
      "Train on 800000 samples, validate on 200000 samples\n",
      "Epoch 1/1\n",
      "800000/800000 [==============================] - 42s - loss: 5.8439 - acc: 0.0939 - val_loss: 5.7967 - val_acc: 0.1111\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x140ce3a20>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train softmax model\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "m_sm.compile(optimizer=sgd,loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "m_sm.fit(x=X,y=Y_sm_half,batch_size=64,nb_epoch=2,validation_split=0.2)\n",
    "sgd = SGD(lr=0.001, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "m_sm.layers[2] = Dropout(0.)\n",
    "m_sm.layers[5] = Dropout(0.)\n",
    "m_sm.compile(optimizer=sgd,loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "m_sm.fit(x=X,y=Y_sm_half,batch_size=64,nb_epoch=1,validation_split=0.2)\n",
    "sgd = SGD(lr=0.0001, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "m_sm.compile(optimizer=sgd,loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "m_sm.fit(x=X,y=Y_sm_half,batch_size=64,nb_epoch=1,validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 3.17538571  3.24850011  4.63944006  4.9945178   4.85439539]\n",
      "[5 4 1 4 7]\n",
      "[ 4.29260015  4.30029583  5.08920574  4.69491196  4.74303722]\n",
      "[2 9 0 9 8]\n",
      "mean proportion error 0.13\n"
     ]
    }
   ],
   "source": [
    "k = 100\n",
    "y_pred = m_reg.predict(X[0:k,:])\n",
    "y1_pred = y_pred[:,0]\n",
    "y2_pred = y_pred[:,1]\n",
    "y1_true = Y_reg[:k,0]\n",
    "y2_true = Y_reg[:k,1]\n",
    "print(y1_pred[0:5])\n",
    "print(y1_true[0:5])\n",
    "print(y2_pred[0:5])\n",
    "print(y2_true[0:5])\n",
    "\n",
    "mae = (np.abs(y1_pred-y1_true) + np.abs(y2_pred-y2_pred)).sum()/2/k/(w-1)\n",
    "print('mean proportion error %.2f' % mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 32/100 [========>.....................] - ETA: 0s[0 4 2 6 4 6 2 2 0 2 2 2 0 0 5 2 2 6 1 0 3 0 0 0 5 1 2 0 2 3 2 0 5 2 2 1 0\n",
      " 5 2 1 1 1 2 4 6 4 3 2 0 4 1 0 3 1 4 3 0 5 0 2 1 2 4 0 2 3 6 5 1 0 0 3 0 0\n",
      " 1 3 2 5 3 1 3 2 2 2 0 0 1 1 1 2 0 4 2 1 2 3 2 0 1 2]\n",
      "[5 4 1 4 7 9 1 6 6 7 2 2 2 1 2 5 0 8 1 5 3 2 5 6 6 3 2 4 3 3 9 0 5 6 0 1 9\n",
      " 5 0 6 8 2 0 1 5 6 4 2 8 4 8 5 3 9 0 6 6 3 7 2 6 9 2 9 2 4 6 9 3 8 2 5 8 7\n",
      " 2 1 9 4 1 6 2 6 7 6 5 8 1 4 6 3 1 7 3 7 7 7 6 5 8 8]\n",
      "[8 4 9 9 1 9 2 9 5 4 8 9 2 5 3 1 6 0 0 1 8 4 9 6 5 3 1 2 8 9 2 7 8 6 1 0 9\n",
      " 3 9 1 7 4 0 3 7 8 9 2 1 0 5 4 9 0 6 7 7 0 7 7 7 4 4 0 7 7 7 0 5 0 9 6 6 5\n",
      " 0 3 2 6 3 6 9 9 6 1 1 5 8 8 3 8 2 5 5 7 8 8 0 4 3 8]\n",
      "[0 7 9 9 6 9 9 7 4 1 4 2 6 3 0 6 8 4 3 3 4 3 5 0 6 2 8 8 7 0 3 3 4 4 0 0 2\n",
      " 1 0 5 1 1 2 0 2 3 7 9 4 8 1 3 2 5 5 3 7 0 5 8 2 3 2 2 2 4 3 4 8 8 2 4 0 8\n",
      " 5 8 7 1 2 2 3 2 7 0 9 5 4 3 4 3 1 2 8 0 3 6 4 4 4 7]\n",
      "accuracy 0.130\n",
      "mean proportion error 0.18\n"
     ]
    }
   ],
   "source": [
    "k = 100\n",
    "y_pred = m_sm.predict_proba(X[0:k,:])\n",
    "y1_pred = y_pred[:,0:w].argmax(axis=1)\n",
    "y2_pred = y_pred[:,w:(2*w)].argmax(axis=1)\n",
    "y1_true = Y_sm[0:k].argmax(axis=1)\n",
    "y2_true = Y_sm[k:(2*k)].argmax(axis=1)\n",
    "print(y1_pred)\n",
    "print(y1_true)\n",
    "print(y2_pred)\n",
    "print(y2_true)\n",
    "acc = ((y1_pred==y1_true).sum()+(y2_pred==y2_true).sum())/2/k\n",
    "print('accuracy %.3f' % acc)\n",
    "mae = (np.abs(y1_pred-y1_true) + np.abs(y2_pred-y2_pred)).sum()/2/k/(w-1)\n",
    "print('mean proportion error %.2f' % mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen in the above 2 comparisons, the regression and single softmax doesn't seem to work very well in detecting the 'max pixel' from a w\\*w grid. We now try augmenting the final output with 2 different softmaxes, 1 for each dimension. This will require the use of the functional api, since we're forking the outputs from the fc layer into 2 softmaxes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(0)\n",
    "n = 1000000\n",
    "h = 6\n",
    "w = 10\n",
    "X = np.round(rng.uniform(low=-1,high=1,size=(n,h*w)),decimals=2)\n",
    "\n",
    "# prepare Y outputs for regression\n",
    "y1_reg = np.int16(X.argmax(axis=1)/w).reshape((n,1)) # h, y\n",
    "y2_reg = np.int16(X.argmax(axis=1)%w).reshape((n,1)) # w, x\n",
    "\n",
    "# prepare Y outputs for softmax\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc = OneHotEncoder(sparse=False)\n",
    "enc.fit(np.arange(h).reshape(h,1))\n",
    "y1_sm = enc.transform(y1_reg)\n",
    "enc.fit(np.arange(w).reshape(w,1))\n",
    "y2_sm = enc.transform(y2_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 0.1   0.43  0.21  0.09 -0.15  0.29 -0.12  0.78  0.93 -0.23]\n",
      "  [ 0.58  0.06  0.14  0.85 -0.86 -0.83 -0.96  0.67  0.56  0.74]\n",
      "  [ 0.96  0.6  -0.08  0.56 -0.76  0.28 -0.71  0.89  0.04 -0.17]\n",
      "  [-0.47  0.55 -0.09  0.14 -0.96  0.24  0.22  0.23  0.89  0.36]\n",
      "  [-0.28 -0.13  0.4  -0.88  0.33  0.34 -0.58 -0.74 -0.37 -0.27]\n",
      "  [ 0.14 -0.12  0.98 -0.8  -0.58 -0.68  0.31 -0.49 -0.07 -0.51]]\n",
      "\n",
      " [[-0.68 -0.78  0.31 -0.72 -0.61 -0.26  0.64 -0.81  0.68 -0.81]\n",
      "  [ 0.95 -0.06  0.95  0.21  0.48 -0.92 -0.43 -0.76 -0.41 -0.76]\n",
      "  [-0.36 -0.17 -0.87  0.38  0.13 -0.47  0.05 -0.81  0.15  0.86]\n",
      "  [-0.36  0.33 -0.74  0.43 -0.42 -0.63  0.17 -0.96  0.66 -0.99]\n",
      "  [ 0.36 -0.46  0.47  0.92 -0.5   0.15  0.18  0.14 -0.55  0.91]\n",
      "  [-0.11  0.69  0.4  -0.41  0.63 -0.21  0.76  0.16  0.76  0.39]]\n",
      "\n",
      " [[ 0.45  0.    0.91  0.29 -0.15  0.21 -0.96 -0.4   0.32 -0.42]\n",
      "  [ 0.24 -0.14 -0.73 -0.4   0.14  0.18  0.15  0.31  0.3  -0.14]\n",
      "  [ 0.79 -0.26 -0.13  0.78  0.61  0.41 -0.8   0.84  0.43  1.  ]\n",
      "  [-0.7   0.74 -0.68  0.23 -0.75  0.7   0.61  0.14 -0.19 -0.86]\n",
      "  [ 0.39 -0.09  0.44  0.73  0.95  0.71 -0.98 -0.28  0.46 -0.66]\n",
      "  [ 0.04 -0.89 -0.6  -0.96  0.59 -0.55 -0.31  0.86  0.41 -0.94]]\n",
      "\n",
      " [[-0.67  0.24  0.15 -0.52  0.87  0.23  0.07  0.18  0.46 -0.38]\n",
      "  [-0.2  -0.58 -0.63  0.89  0.48 -0.02 -0.55 -0.49 -0.88 -0.13]\n",
      "  [-0.38  0.39 -0.24 -0.64 -0.95 -0.87  0.36 -0.09  0.07  0.79]\n",
      "  [ 0.98 -0.57  0.33 -0.47 -0.96  0.52 -0.36 -0.23  0.18  0.66]\n",
      "  [ 0.26  0.75 -0.45  0.6  -0.63  0.91  0.37 -0.57  0.89  0.46]\n",
      "  [-0.49 -0.57  0.04 -0.95 -0.59 -0.15 -0.25 -0.07 -0.44  0.17]]\n",
      "\n",
      " [[ 0.73 -0.76  0.03 -0.74  0.43 -0.21  0.13 -0.63 -0.71 -0.02]\n",
      "  [-0.29  0.88  0.53  0.5   0.81 -0.83  0.1   0.17  0.92 -0.42]\n",
      "  [-0.52 -0.8  -0.97  0.86  0.34  0.57 -0.44  0.17 -0.87 -0.03]\n",
      "  [ 0.95  0.75 -0.32  0.92 -0.54  0.9   0.88  0.6   0.26  0.75]\n",
      "  [-0.41  0.7   0.24 -0.97 -0.31 -0.7   0.96 -0.04 -0.01  0.28]\n",
      "  [-0.26 -0.73  0.64 -0.62  0.02 -0.55 -0.8   0.72  0.95  0.92]]]\n",
      "[[5]\n",
      " [1]\n",
      " [2]\n",
      " [3]\n",
      " [4]]\n",
      "[[2]\n",
      " [0]\n",
      " [9]\n",
      " [0]\n",
      " [6]]\n",
      "[[ 0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  1.  0.  0.  0.  0.]\n",
      " [ 0.  0.  1.  0.  0.  0.]\n",
      " [ 0.  0.  0.  1.  0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  0.]]\n",
      "[[ 0.  0.  1.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1.  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "print(X[0:5,:].reshape((5,h,w)))\n",
    "print(y1_reg[0:5,:])\n",
    "print(y2_reg[0:5,:])\n",
    "print(y1_sm[0:5,:])\n",
    "print(y2_sm[0:5,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.layers import Input, Convolution2D, MaxPooling2D, ZeroPadding2D, Layer\n",
    "from keras.layers import Activation, Dropout, Flatten, MaxoutDense, Dense\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "\n",
    "# custom layer for getting max for over each vertical strip\n",
    "class VerticalMax(Layer):\n",
    "    def get_output_shape_for(self, input_shape):\n",
    "        shape = list(input_shape)\n",
    "        assert len(shape) == 4  # only valid for 4D tensors (batch_size,h,w,ch)\n",
    "        return (input_shape[0],input_shape[2],input_shape[3]) # (batch_size,w,ch)\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        return K.max(x, axis=1, keepdims=False)\n",
    "\n",
    "    \n",
    "# custom layer for getting max for over each horizontal strip\n",
    "class HorizontalMax(Layer):\n",
    "    def get_output_shape_for(self, input_shape):\n",
    "        shape = list(input_shape)\n",
    "        assert len(shape) == 4  # only valid for 4D tensors (batch_size,h,w,ch)\n",
    "        return (input_shape[0],input_shape[1],input_shape[3]) # (batch_size,h,ch)\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        return K.max(x, axis=2, keepdims=False)\n",
    "\n",
    "X_conv = X.reshape((n,h,w,1))\n",
    "\n",
    "inputs = Input(shape=(h,w,1))\n",
    "\n",
    "x_pool = VerticalMax()(inputs)\n",
    "x_flat = Flatten()(x_pool)\n",
    "x_dense = Dense(16, activation='relu')(x_flat)\n",
    "x_out = Dense(w, activation='softmax')(x_dense)\n",
    "\n",
    "y_pool = HorizontalMax()(inputs)\n",
    "y_flat = Flatten()(y_pool)\n",
    "y_dense = Dense(16, activation='relu')(y_flat)\n",
    "y_out = Dense(h, activation='softmax')(y_dense)\n",
    "\n",
    "m2_sm = Model(input=[inputs], output=[y_out,x_out])\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "m2_sm.compile(optimizer=sgd, loss='binary_crossentropy',\n",
    "              loss_weights=[0.5, 0.5], metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 800000 samples, validate on 200000 samples\n",
      "Epoch 1/5\n",
      "800000/800000 [==============================] - 65s - loss: 0.3176 - dense_30_loss: 0.3318 - dense_28_loss: 0.3034 - dense_30_acc: 0.8524 - dense_28_acc: 0.9000 - val_loss: 0.2444 - val_dense_30_loss: 0.2095 - val_dense_28_loss: 0.2793 - val_dense_30_acc: 0.9115 - val_dense_28_acc: 0.9000\n",
      "Epoch 2/5\n",
      "800000/800000 [==============================] - 65s - loss: 0.2061 - dense_30_loss: 0.1571 - dense_28_loss: 0.2550 - dense_30_acc: 0.9488 - dense_28_acc: 0.9015 - val_loss: 0.1794 - val_dense_30_loss: 0.1233 - val_dense_28_loss: 0.2354 - val_dense_30_acc: 0.9662 - val_dense_28_acc: 0.9044\n",
      "Epoch 3/5\n",
      "800000/800000 [==============================] - 64s - loss: 0.1656 - dense_30_loss: 0.1058 - dense_28_loss: 0.2254 - dense_30_acc: 0.9760 - dense_28_acc: 0.9083 - val_loss: 0.1543 - val_dense_30_loss: 0.0919 - val_dense_28_loss: 0.2167 - val_dense_30_acc: 0.9816 - val_dense_28_acc: 0.9115\n",
      "Epoch 4/5\n",
      "800000/800000 [==============================] - 59s - loss: 0.1450 - dense_30_loss: 0.0838 - dense_28_loss: 0.2063 - dense_30_acc: 0.9842 - dense_28_acc: 0.9164 - val_loss: 0.1375 - val_dense_30_loss: 0.0770 - val_dense_28_loss: 0.1980 - val_dense_30_acc: 0.9864 - val_dense_28_acc: 0.9205\n",
      "Epoch 5/5\n",
      "800000/800000 [==============================] - 359s - loss: 0.1320 - dense_30_loss: 0.0711 - dense_28_loss: 0.1930 - dense_30_acc: 0.9880 - dense_28_acc: 0.9235 - val_loss: 0.1277 - val_dense_30_loss: 0.0667 - val_dense_28_loss: 0.1887 - val_dense_30_acc: 0.9879 - val_dense_28_acc: 0.9260\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x178206c88>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m2_sm.fit(x=[X_conv],y=[y1_sm,y2_sm],batch_size=64,nb_epoch=5,validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5 1 2 3 4 4 1 5 0 5 1 2 4 1 2 1 1 0 4 3 2 3 1 4 2 5 2 0 1 4 1 0 3 5 1 2 1\n",
      " 3 5 2 2 0 1 4 1 4 5 1 5 3 1 4 1 5 1 2 0 0 3 5 4 3 3 2 1 4 0 0 0 3 0 5 2 5\n",
      " 1 4 4 2 4 4 5 2 2 1 5 5 0 1 3 5 0 0 4 3 2 3 0 5 4 3]\n",
      "[5 1 2 3 4 4 1 5 0 5 1 2 4 1 2 1 1 0 4 3 2 3 1 4 2 5 2 0 1 4 1 0 3 5 1 2 1\n",
      " 3 4 2 0 0 1 4 1 4 5 1 5 3 1 4 1 5 1 2 0 0 3 5 4 3 3 2 1 4 0 0 0 3 0 5 2 5\n",
      " 1 4 4 2 4 4 5 2 2 1 4 5 0 1 3 5 0 0 4 3 2 3 0 5 4 3]\n",
      "[2 0 4 0 9 8 7 9 8 5 2 8 9 4 5 2 4 8 5 9 2 9 2 7 0 1 9 4 0 2 9 2 7 1 0 7 3\n",
      " 0 2 7 5 2 5 2 0 4 2 1 1 0 7 3 8 5 2 5 3 8 0 8 1 3 8 1 9 0 8 7 7 6 0 3 8 3\n",
      " 5 2 7 1 2 7 1 1 8 7 1 5 8 9 2 0 1 4 7 8 8 8 0 7 0 2]\n",
      "[2 0 9 0 6 9 7 8 8 5 9 8 6 4 5 2 4 8 5 4 2 8 2 7 0 1 8 6 0 2 9 8 7 1 0 7 3\n",
      " 0 2 6 5 8 5 2 4 9 2 6 1 9 7 3 8 6 2 5 3 8 0 8 4 3 8 1 9 0 9 6 7 6 6 3 6 3\n",
      " 5 2 7 1 2 7 6 1 8 9 1 5 6 4 2 0 1 6 7 8 8 8 0 7 0 2]\n",
      "accuracy 0.845\n",
      "mean proportion error for x 0.00\n",
      "mean proportion error for y 0.00\n"
     ]
    }
   ],
   "source": [
    "k = 100\n",
    "y_pred = m2_sm.predict(X_conv[0:k,...])\n",
    "y1_pred = y_pred[0].argmax(axis=1)\n",
    "y2_pred = y_pred[1].argmax(axis=1)\n",
    "y1_true = y1_sm[0:k].argmax(axis=1)\n",
    "y2_true = y2_sm[0:k].argmax(axis=1)\n",
    "print(y1_pred)\n",
    "print(y1_true)\n",
    "print(y2_pred)\n",
    "print(y2_true)\n",
    "acc = ((y1_pred==y1_true).sum()+(y2_pred==y2_true).sum())/2/k\n",
    "print('accuracy %.3f' % acc)\n",
    "mae_x = (np.abs(y1_pred-y1_true)).sum()/k/(w-1)\n",
    "mae_y = (np.abs(y2_pred-y2_pred)).sum()/k/(h-1)\n",
    "print('mean proportion error for x %.2f' % mae_x)\n",
    "print('mean proportion error for y %.2f' % mae_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What crazy improvement! I think the results speak for themselves. Nevertheless, let's try a regression model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputs = Input(shape=(h,w,1))\n",
    "\n",
    "x_pool = VerticalMax()(inputs)\n",
    "x_flat = Flatten()(x_pool)\n",
    "x_dense = Dense(16, activation='relu')(x_flat)\n",
    "x_out = Dense(1, activation='relu')(x_dense)\n",
    "\n",
    "y_pool = HorizontalMax()(inputs)\n",
    "y_flat = Flatten()(y_pool)\n",
    "y_dense = Dense(16, activation='relu')(y_flat)\n",
    "y_out = Dense(1, activation='relu')(y_dense)\n",
    "\n",
    "m2_reg = Model(input=[inputs], output=[y_out,x_out])\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "m2_reg.compile(optimizer=sgd, loss='mean_squared_error',\n",
    "              loss_weights=[0.5, 0.5], metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 800000 samples, validate on 200000 samples\n",
      "Epoch 1/5\n",
      "800000/800000 [==============================] - 49s - loss: 3.3301 - dense_26_loss: 1.4280 - dense_24_loss: 5.2323 - dense_26_mean_absolute_error: 0.9443 - dense_24_mean_absolute_error: 1.8889 - val_loss: 3.0663 - val_dense_26_loss: 1.2119 - val_dense_24_loss: 4.9208 - val_dense_26_mean_absolute_error: 0.8596 - val_dense_24_mean_absolute_error: 1.7541\n",
      "Epoch 2/5\n",
      "800000/800000 [==============================] - 51s - loss: 2.7028 - dense_26_loss: 1.1716 - dense_24_loss: 4.2340 - dense_26_mean_absolute_error: 0.8265 - dense_24_mean_absolute_error: 1.6496 - val_loss: 2.6024 - val_dense_26_loss: 1.1014 - val_dense_24_loss: 4.1034 - val_dense_26_mean_absolute_error: 0.8076 - val_dense_24_mean_absolute_error: 1.6019\n",
      "Epoch 3/5\n",
      "800000/800000 [==============================] - 48s - loss: 2.5532 - dense_26_loss: 1.1335 - dense_24_loss: 3.9729 - dense_26_mean_absolute_error: 0.8110 - dense_24_mean_absolute_error: 1.5793 - val_loss: 2.4903 - val_dense_26_loss: 1.1602 - val_dense_24_loss: 3.8205 - val_dense_26_mean_absolute_error: 0.8234 - val_dense_24_mean_absolute_error: 1.5437\n",
      "Epoch 4/5\n",
      "800000/800000 [==============================] - 49s - loss: 2.5010 - dense_26_loss: 1.1120 - dense_24_loss: 3.8900 - dense_26_mean_absolute_error: 0.8028 - dense_24_mean_absolute_error: 1.5577 - val_loss: 2.5275 - val_dense_26_loss: 1.0603 - val_dense_24_loss: 3.9948 - val_dense_26_mean_absolute_error: 0.7911 - val_dense_24_mean_absolute_error: 1.5738\n",
      "Epoch 5/5\n",
      "800000/800000 [==============================] - 47s - loss: 2.4683 - dense_26_loss: 1.0850 - dense_24_loss: 3.8516 - dense_26_mean_absolute_error: 0.7918 - dense_24_mean_absolute_error: 1.5478 - val_loss: 2.4405 - val_dense_26_loss: 1.1343 - val_dense_24_loss: 3.7467 - val_dense_26_mean_absolute_error: 0.8136 - val_dense_24_mean_absolute_error: 1.5353\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x145d35470>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m2_reg.fit(x=[X_conv],y=[y1_reg,y2_reg],batch_size=64,nb_epoch=5,validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((100, 1), (100, 1))"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = m2_reg.predict(X_conv[0:k,...])\n",
    "y_pred[0].shape,y_pred[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3.67926121]\n",
      " [ 1.19287491]\n",
      " [ 1.48898304]\n",
      " [ 3.47848463]\n",
      " [ 3.22325993]]\n",
      "[[5]\n",
      " [1]\n",
      " [2]\n",
      " [3]\n",
      " [4]]\n",
      "[[ 1.1354984 ]\n",
      " [ 1.49048209]\n",
      " [ 7.79505873]\n",
      " [ 0.47571421]\n",
      " [ 4.60573196]]\n",
      "[[2]\n",
      " [0]\n",
      " [9]\n",
      " [0]\n",
      " [6]]\n",
      "accuracy 0.350\n",
      "mean proportion error for x 0.09\n",
      "mean proportion error for y 0.00\n"
     ]
    }
   ],
   "source": [
    "k = 100\n",
    "y1_pred, y2_pred = m2_reg.predict(X_conv[0:k,...])\n",
    "y1_true = y1_reg[0:k]\n",
    "y2_true = y2_reg[0:k]\n",
    "print(y1_pred[:5])\n",
    "print(y1_true[:5])\n",
    "print(y2_pred[:5])\n",
    "print(y2_true[:5])\n",
    "acc = ((np.round(y1_pred)==y1_true).sum()+(np.round(y2_pred)==y2_true).sum())/2/k\n",
    "print('accuracy %.3f' % acc)\n",
    "mae_x = (np.abs(y1_pred-y1_true)).sum()/k/(w-1)\n",
    "mae_y = (np.abs(y2_pred-y2_pred)).sum()/k/(h-1)\n",
    "print('mean proportion error for x %.2f' % mae_x)\n",
    "print('mean proportion error for y %.2f' % mae_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know that this architecture works for such inputs, what happens if we extend the size of the image and the outputs?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
